{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow 2 Complete Project Workflow in Amazon SageMaker\n",
    "### Data Preprocessing -> Code Prototyping -> Automatic Model Tuning -> Deployment\n",
    "    \n",
    "1. [Introduction](#Introduction)\n",
    "2. [SageMaker Processing for dataset transformation](#SageMakerProcessing)\n",
    "3. [Local Mode training](#LocalModeTraining)\n",
    "4. [Local Mode endpoint](#LocalModeEndpoint)\n",
    "5. [SageMaker hosted training](#SageMakerHostedTraining)\n",
    "6. [Automatic Model Tuning](#AutomaticModelTuning)\n",
    "7. [SageMaker hosted endpoint](#SageMakerHostedEndpoint)\n",
    "8. [Workflow Automation with SageMaker Pipelines](#WorkflowAutomation)\n",
    "    1. [Pipeline Parameters](#PipelineParameters)\n",
    "    2. [Processing Step](#ProcessingStep)\n",
    "    3. [Training and Model Creation Steps](#TrainingModelCreation)\n",
    "    4. [Batch Scoring Step](#BatchScoringStep)\n",
    "    5. [Creating and executing the pipeline](#CreatingExecutingPipeline)\n",
    "9. [ML Lineage Tracking](#LineageOfPipelineArtifacts)\n",
    "10. [Extensions](#Extensions)\n",
    "\n",
    "\n",
    "***Prerequisites:***\n",
    "- In SageMaker Studio, for kernel select **Python 3 (TensorFlow 2.3 Python 3.7 CPU Optimized)**; for a SageMaker Notebook Instance, select the kernel **conda_tensorflow2_py36**.\n",
    "- If you're using SageMaker Studio, skip the Local Mode sections of this example (those sections work with a SageMaker Notebook Instance).\n",
    "\n",
    "    \n",
    "## Introduction <a class=\"anchor\" id=\"Introduction\">\n",
    "\n",
    "If you are using TensorFlow 2, you can use the Amazon SageMaker prebuilt TensorFlow 2 framework container with training scripts similar to those you would use outside SageMaker. This feature is named Script Mode.  Using Script Mode and other SageMaker features, you can build a complete workflow for a TensorFlow 2 project.  This notebook presents such a workflow, including all key steps such as preprocessing data with SageMaker Processing, model training with SageMaker hosted training, model tuning with SageMaker Automatic Model Tuning, and production-ready model deployment with SageMaker hosted endpoints. \n",
    "\n",
    "Working through these steps in a notebook is part of the prototyping process; however, a repeatable production workflow typically is run outside notebooks. To demonstrate automating the workflow, we'll use [Amazon SageMaker Pipelines](https://aws.amazon.com/sagemaker/pipelines) for workflow orchestration. Purpose-built for machine learning (ML), SageMaker Pipelines helps you automate different steps of the ML workflow including data processing, model training, and batch prediction (scoring), and apply conditions such as approvals for model quality. It also includes a model registry and model lineage tracker.   \n",
    "\n",
    "To enable you to run this notebook within a reasonable time (typically less than an hour), this notebook's use case is a straightforward regression task:  predicting house prices based on the well-known Boston Housing dataset. This public dataset contains 13 features regarding housing stock of towns in the Boston area.  Features include average number of rooms, accessibility to radial highways, adjacency to a major river, etc.  \n",
    "\n",
    "To begin, we'll import some necessary packages and set up directories for local training and test data.  We'll also set up a SageMaker Session to perform various operations, and specify an Amazon S3 bucket to hold input data and output.  The default bucket used here is created by SageMaker if it doesn't already exist, and named in accordance with the AWS account ID and AWS Region.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import sagemaker\n",
    "import tensorflow as tf\n",
    "\n",
    "sess = sagemaker.session.Session()\n",
    "bucket = sess.default_bucket() \n",
    "region = boto3.Session().region_name\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "train_dir = os.path.join(os.getcwd(), 'data/train')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "test_dir = os.path.join(os.getcwd(), 'data/test')\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "raw_dir = os.path.join(os.getcwd(), 'data/raw')\n",
    "os.makedirs(raw_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Processing for dataset transformation <a class=\"anchor\" id=\"SageMakerProcessing\">\n",
    "\n",
    "Next, we'll import the dataset and transform it with SageMaker Processing, which can be used to process terabytes of data in a SageMaker-managed cluster separate from the instance running your notebook server. In a typical SageMaker workflow, ***notebooks are only used for prototyping*** and can be run on relatively inexpensive and less powerful instances, while full-scale processing, training and model hosting tasks are run on separate, more powerful SageMaker-managed instances.  SageMaker Processing includes off-the-shelf support for Scikit-learn and Spark, as well as a Bring Your Own Container option, so it can be used with many different data transformation technologies and tasks.  Another SageMaker feature related to data processing is [SageMaker Data Wrangler](https://aws.amazon.com/sagemaker/data-wrangler/), a visual data preparation tool integrated with the SageMaker Studio UI.    \n",
    "\n",
    "To work with SageMaker Processing, first we'll load the Boston Housing dataset, save the raw feature data and upload it to Amazon S3 so it can be accessed by SageMaker Processing.  We'll also save the labels for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-154082585954/tf-2-workflow/data/raw\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.python.keras.datasets import boston_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "np.save(os.path.join(raw_dir, 'x_train.npy'), x_train)\n",
    "np.save(os.path.join(raw_dir, 'x_test.npy'), x_test)\n",
    "np.save(os.path.join(raw_dir, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(raw_dir, 'y_test.npy'), y_test)\n",
    "s3_prefix = 'tf-2-workflow'\n",
    "rawdata_s3_prefix = '{}/data/raw'.format(s3_prefix)\n",
    "raw_s3 = sess.upload_data(path='./data/raw/', key_prefix=rawdata_s3_prefix)\n",
    "print(raw_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, simply supply an ordinary Python data preprocessing script as shown below.  For this example, we're using a SageMaker prebuilt Scikit-learn framework container, which includes many common functions for processing data.  There are few limitations on what kinds of code and operations you can run, and only a minimal API contract:  input and output data must be placed in specified directories.  If this is done, SageMaker Processing automatically loads the input data from S3 and uploads transformed data back to S3 when the job is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    input_files = glob.glob('{}/*.npy'.format('/opt/ml/processing/input'))\n",
    "    print('\\nINPUT FILE LIST: \\n{}\\n'.format(input_files))\n",
    "    scaler = StandardScaler()\n",
    "    for file in input_files:\n",
    "        raw = np.load(file)\n",
    "        # only transform feature columns\n",
    "        if 'y_' not in file:\n",
    "            transformed = scaler.fit_transform(raw)\n",
    "        if 'train' in file:\n",
    "            if 'y_' in file:\n",
    "                output_path = os.path.join('/opt/ml/processing/train', 'y_train.npy')\n",
    "                np.save(output_path, raw)\n",
    "                print('SAVED LABEL TRAINING DATA FILE\\n')\n",
    "            else:\n",
    "                output_path = os.path.join('/opt/ml/processing/train', 'x_train.npy')\n",
    "                np.save(output_path, transformed)\n",
    "                print('SAVED TRANSFORMED TRAINING DATA FILE\\n')\n",
    "        else:\n",
    "            if 'y_' in file:\n",
    "                output_path = os.path.join('/opt/ml/processing/test', 'y_test.npy')\n",
    "                np.save(output_path, raw)\n",
    "                print('SAVED LABEL TEST DATA FILE\\n')\n",
    "            else:\n",
    "                output_path = os.path.join('/opt/ml/processing/test', 'x_test.npy')\n",
    "                np.save(output_path, transformed)\n",
    "                print('SAVED TRANSFORMED TEST DATA FILE\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the SageMaker Processing job, we instantiate a `SKLearnProcessor` object.  This object allows you to specify the instance type to use in the job, as well as how many instances.  Although the Boston Housing dataset is quite small, we'll use two instances to showcase how easy it is to spin up a cluster for SageMaker Processing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "sklearn_processor1 = SKLearnProcessor(framework_version='0.23-1',\n",
    "                                     role=get_execution_role(),\n",
    "                                     instance_type='ml.m5.xlarge',\n",
    "                                     instance_count=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to run the Processing job.  To enable distributing the data files equally among the instances, we specify the `ShardedByS3Key` distribution type in the `ProcessingInput` object.  This ensures that if we have `n` instances, each instance will receive `1/n` files from the specified S3 bucket.  It may take around 3 minutes for the following code cell to run, mainly to set up the cluster.  At the end of the job, the cluster automatically will be torn down by SageMaker.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  tf-2-workflow-18-22-49-17\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-154082585954/tf-2-workflow/data/raw', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-154082585954/tf-2-workflow-18-22-49-17/input/code/preprocessing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'train', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-154082585954/tf-2-workflow/data/train', 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'test', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-154082585954/tf-2-workflow/data/test', 'LocalPath': '/opt/ml/processing/test', 'S3UploadMode': 'EndOfJob'}}]\n",
      "...........................\u001b[34mINPUT FILE LIST: \u001b[0m\n",
      "\u001b[34m['/opt/ml/processing/input/x_test.npy', '/opt/ml/processing/input/y_test.npy']\n",
      "\u001b[0m\n",
      "\u001b[34mSAVED TRANSFORMED TEST DATA FILE\n",
      "\u001b[0m\n",
      "\u001b[34mSAVED LABEL TEST DATA FILE\n",
      "\u001b[0m\n",
      "\u001b[35mINPUT FILE LIST: \u001b[0m\n",
      "\u001b[35m['/opt/ml/processing/input/x_train.npy', '/opt/ml/processing/input/y_train.npy']\n",
      "\u001b[0m\n",
      "\u001b[35mSAVED TRANSFORMED TRAINING DATA FILE\n",
      "\u001b[0m\n",
      "\u001b[35mSAVED LABEL TRAINING DATA FILE\n",
      "\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from time import gmtime, strftime \n",
    "\n",
    "processing_job_name = \"tf-2-workflow-{}\".format(strftime(\"%d-%H-%M-%S\", gmtime()))\n",
    "output_destination = 's3://{}/{}/data'.format(bucket, s3_prefix)\n",
    "\n",
    "sklearn_processor1.run(code='preprocessing.py',\n",
    "                      job_name=processing_job_name,\n",
    "                      inputs=[ProcessingInput(\n",
    "                        source=raw_s3,\n",
    "                        destination='/opt/ml/processing/input',\n",
    "                        s3_data_distribution_type='ShardedByS3Key')],\n",
    "                      outputs=[ProcessingOutput(output_name='train',\n",
    "                                                destination='{}/train'.format(output_destination),\n",
    "                                                source='/opt/ml/processing/train'),\n",
    "                               ProcessingOutput(output_name='test',\n",
    "                                                destination='{}/test'.format(output_destination),\n",
    "                                                source='/opt/ml/processing/test')])\n",
    "\n",
    "preprocessing_job_description = sklearn_processor1.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the log output of the SageMaker Processing job above, you should be able to see logs in two different colors for the two different instances, and that each instance received different files.  Without the `ShardedByS3Key` distribution type, each instance would have received a copy of **all** files.  By spreading the data equally among `n` instances, you should receive a speedup by approximately a factor of `n` for most stateless data transformations.  After saving the job results locally, we'll move on to prototyping training and inference code with Local Mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-154082585954/tf-2-workflow/data/train/x_train.npy to data/train/x_train.npy\n",
      "download: s3://sagemaker-us-east-1-154082585954/tf-2-workflow/data/train/y_train.npy to data/train/y_train.npy\n",
      "download: s3://sagemaker-us-east-1-154082585954/tf-2-workflow/data/test/x_test.npy to data/test/x_test.npy\n",
      "download: s3://sagemaker-us-east-1-154082585954/tf-2-workflow/data/test/y_test.npy to data/test/y_test.npy\n"
     ]
    }
   ],
   "source": [
    "x_train_in_s3 = '{}/train/x_train.npy'.format(output_destination)\n",
    "y_train_in_s3 = '{}/train/y_train.npy'.format(output_destination)\n",
    "x_test_in_s3 = '{}/test/x_test.npy'.format(output_destination)\n",
    "y_test_in_s3 = '{}/test/y_test.npy'.format(output_destination)\n",
    "\n",
    "!aws s3 cp {x_train_in_s3} ./data/train/x_train.npy\n",
    "!aws s3 cp {y_train_in_s3} ./data/train/y_train.npy\n",
    "!aws s3 cp {x_test_in_s3} ./data/test/x_test.npy\n",
    "!aws s3 cp {y_test_in_s3} ./data/test/y_test.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SageMaker hosted training <a class=\"anchor\" id=\"SageMakerHostedTraining\">\n",
    "\n",
    "Once the prototyping phase of a project is complete, the hosted training feature of SageMaker is preferred for training jobs, especially large-scale, distributed training.  Unlike training in a notebook environment or SageMaker Local Mode, for hosted training the actual training itself occurs not in the notebook environment, but on a separate cluster of potentially more powerful machines managed by SageMaker.  Before starting hosted training, the data must be in S3, or an Amazon EFS or Amazon FSx for Lustre file system. We'll upload to S3 now, and confirm the upload was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_prefix = 'tf-2-workflow'\n",
    "\n",
    "traindata_s3_prefix = '{}/data/train'.format(s3_prefix)\n",
    "testdata_s3_prefix = '{}/data/test'.format(s3_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 's3://sagemaker-us-east-1-154082585954/tf-2-workflow/data/train', 'test': 's3://sagemaker-us-east-1-154082585954/tf-2-workflow/data/test'}\n"
     ]
    }
   ],
   "source": [
    "train_s3 = sess.upload_data(path='./data/train/', key_prefix=traindata_s3_prefix)\n",
    "test_s3 = sess.upload_data(path='./data/test/', key_prefix=testdata_s3_prefix)\n",
    "\n",
    "inputs = {'train':train_s3, 'test': test_s3}\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to set up an Estimator object for SageMaker hosted training. It is similar to a SageMaker Local Mode Estimator, except the `train_instance_type` has been set to a SageMaker ML instance type instead of `local` for Local Mode, in this case a `c5` compute optimized instance type.  The Git configuration passed into the Estimator pulls the training script from a Git repository and ensures that the team is sharing the same source controlled code instead of having different scripts scattered on various team members' machines. \n",
    "\n",
    "Hyperparameters are passed in as a dictionary with a longer number of epochs than the number used in a project's prototyping phase.  For hosted training initiated after prototyping is complete, you can train the model for a larger number of epochs with the expectation that model training likely will proceed without code-related errors and will converge to an improved, lower validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "train_instance_type = 'ml.c5.xlarge'\n",
    "hyperparameters = {'epochs': 30, 'batch_size': 128, 'learning_rate': 0.01}\n",
    "\n",
    "git_config = {'repo': 'https://github.com/Amirosimani/workshop_18052021', \n",
    "              'branch': 'main'}\n",
    "\n",
    "hosted_estimator = TensorFlow(\n",
    "                       git_config=git_config,\n",
    "                       source_dir='day_2/tf2_smpipelines/train_model',\n",
    "                       entry_point='train.py',\n",
    "                       instance_type=train_instance_type,\n",
    "                       instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=sagemaker.get_execution_role(),\n",
    "                       base_job_name='tf-2-workflow',\n",
    "                       framework_version='2.3.1',\n",
    "                       py_version='py37',\n",
    "                       script_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After starting the hosted training job with the `fit` method call below, you should observe the training converge over the longer number of epochs to a validation loss that is considerably lower than that which can be achieved in a short prototyping test such as a Local Mode training job.  Can we do better than the model produced by this single hosted training job? We'll look into a way to do so in the **Automatic Model Tuning** section below. In the meantime, the hosted training job should take about 3 minutes to complete.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-18 23:35:32 Starting - Starting the training job...\n",
      "2021-05-18 23:35:58 Starting - Launching requested ML instancesProfilerReport-1621380931: InProgress\n",
      "......\n",
      "2021-05-18 23:36:58 Starting - Preparing the instances for training......\n",
      "2021-05-18 23:37:58 Downloading - Downloading input data\n",
      "2021-05-18 23:37:58 Training - Downloading the training image..\u001b[34m2021-05-18 23:38:09.061350: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-05-18 23:38:09.066742: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2021-05-18 23:38:09.245871: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-05-18 23:38:12,425 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2021-05-18 23:38:12,432 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-05-18 23:38:12,932 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-05-18 23:38:12,958 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-05-18 23:38:12,974 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-05-18 23:38:12,987 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 128,\n",
      "        \"model_dir\": \"s3://sagemaker-us-east-1-154082585954/tf-2-workflow-2021-05-18-23-35-30-546/model\",\n",
      "        \"epochs\": 30,\n",
      "        \"learning_rate\": 0.01\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tf-2-workflow-2021-05-18-23-35-30-546\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-154082585954/tf-2-workflow-2021-05-18-23-35-30-546/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":128,\"epochs\":30,\"learning_rate\":0.01,\"model_dir\":\"s3://sagemaker-us-east-1-154082585954/tf-2-workflow-2021-05-18-23-35-30-546/model\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-154082585954/tf-2-workflow-2021-05-18-23-35-30-546/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":128,\"epochs\":30,\"learning_rate\":0.01,\"model_dir\":\"s3://sagemaker-us-east-1-154082585954/tf-2-workflow-2021-05-18-23-35-30-546/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tf-2-workflow-2021-05-18-23-35-30-546\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-154082585954/tf-2-workflow-2021-05-18-23-35-30-546/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"128\",\"--epochs\",\"30\",\"--learning_rate\",\"0.01\",\"--model_dir\",\"s3://sagemaker-us-east-1-154082585954/tf-2-workflow-2021-05-18-23-35-30-546/model\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://sagemaker-us-east-1-154082585954/tf-2-workflow-2021-05-18-23-35-30-546/model\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=30\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.01\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.7 train.py --batch_size 128 --epochs 30 --learning_rate 0.01 --model_dir s3://sagemaker-us-east-1-154082585954/tf-2-workflow-2021-05-18-23-35-30-546/model\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mTraining data location: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mTest data location: /opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mx train (404, 13) y train (404,)\u001b[0m\n",
      "\u001b[34mx test (102, 13) y test (102,)\u001b[0m\n",
      "\u001b[34m/cpu:0\u001b[0m\n",
      "\u001b[34mbatch_size = 128, epochs = 30, learning rate = 0.01\u001b[0m\n",
      "\u001b[34m[2021-05-18 23:38:14.853 ip-10-2-229-243.ec2.internal:24 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2021-05-18 23:38:15.076 ip-10-2-229-243.ec2.internal:24 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2021-05-18 23:38:15.178 ip-10-2-229-243.ec2.internal:24 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-05-18 23:38:15.178 ip-10-2-229-243.ec2.internal:24 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-05-18 23:38:15.179 ip-10-2-229-243.ec2.internal:24 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-05-18 23:38:15.179 ip-10-2-229-243.ec2.internal:24 INFO state_store.py:75] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-05-18 23:38:15.181 ip-10-2-229-243.ec2.internal:24 INFO hook.py:413] Monitoring the collections: metrics, sm_metrics, losses\u001b[0m\n",
      "\u001b[34mEpoch 1/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 633.4668 - batch: 0.0000e+00#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 52ms/step - loss: 600.7057 - batch: 0.0000e+00 - val_loss: 495.2780 - val_batch: 0.0000e+00\u001b[0m\n",
      "\u001b[34mEpoch 2/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 530.0222#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 11ms/step - loss: 426.9545 - val_loss: 340.3886 - batch: 1.0000\u001b[0m\n",
      "\u001b[34mEpoch 3/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 321.6736#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 12ms/step - loss: 283.3852 - val_loss: 210.3752 - batch: 2.0000\u001b[0m\n",
      "\u001b[34mEpoch 4/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 182.2919#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 13ms/step - loss: 170.2525 - val_loss: 123.7231 - batch: 3.0000\u001b[0m\n",
      "\u001b[34mEpoch 5/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 115.1649#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 13ms/step - loss: 102.8473 - val_loss: 81.0734 - batch: 4.0000\u001b[0m\n",
      "\u001b[34mEpoch 6/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 75.4301#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 13ms/step - loss: 73.4811 - val_loss: 65.7241 - batch: 5.0000\u001b[0m\n",
      "\u001b[34mEpoch 7/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 65.6792#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 17ms/step - loss: 62.9141 - val_loss: 57.2326 - batch: 6.0000\u001b[0m\n",
      "\u001b[34mEpoch 8/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 53.3737#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 14ms/step - loss: 56.5895 - val_loss: 50.8861 - batch: 7.0000\u001b[0m\n",
      "\u001b[34mEpoch 9/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 56.8251#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 13ms/step - loss: 52.5120 - val_loss: 49.2373 - batch: 8.0000\u001b[0m\n",
      "\u001b[34mEpoch 10/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 27.0476#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 13ms/step - loss: 49.4934 - val_loss: 45.3499 - batch: 9.0000\u001b[0m\n",
      "\u001b[34mEpoch 11/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 51.6214#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 13ms/step - loss: 46.9664 - val_loss: 42.5437 - batch: 10.0000\u001b[0m\n",
      "\u001b[34mEpoch 12/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 39.8037#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 13ms/step - loss: 44.8981 - val_loss: 40.6825 - batch: 11.0000\u001b[0m\n",
      "\u001b[34mEpoch 13/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 58.1443#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 12ms/step - loss: 43.1477 - val_loss: 39.7572 - batch: 12.0000\u001b[0m\n",
      "\u001b[34mEpoch 14/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 39.8981#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 14ms/step - loss: 41.9468 - val_loss: 37.5015 - batch: 13.0000\u001b[0m\n",
      "\u001b[34mEpoch 15/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 28.2866#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 13ms/step - loss: 38.6593 - val_loss: 35.7352 - batch: 14.0000\u001b[0m\n",
      "\u001b[34mEpoch 16/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 32.3282#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 13ms/step - loss: 37.2123 - val_loss: 34.9712 - batch: 15.0000\u001b[0m\n",
      "\u001b[34mEpoch 17/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 46.8194#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 19ms/step - loss: 34.8354 - val_loss: 35.0596 - batch: 16.0000\u001b[0m\n",
      "\u001b[34mEpoch 18/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 46.1998#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 14ms/step - loss: 34.0435 - val_loss: 32.4787 - batch: 17.0000\u001b[0m\n",
      "\u001b[34mEpoch 19/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 34.4418#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 12ms/step - loss: 32.1585 - val_loss: 33.7410 - batch: 18.0000\u001b[0m\n",
      "\u001b[34mEpoch 20/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 39.5199#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 10ms/step - loss: 31.9563 - val_loss: 30.5969 - batch: 19.0000\u001b[0m\n",
      "\u001b[34mEpoch 21/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 26.4783#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 12ms/step - loss: 29.7831 - val_loss: 29.4880 - batch: 20.0000\u001b[0m\n",
      "\u001b[34mEpoch 22/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 26.1165#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 13ms/step - loss: 28.4128 - val_loss: 28.9210 - batch: 21.0000\u001b[0m\n",
      "\u001b[34mEpoch 23/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 20.6940#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 13ms/step - loss: 27.2020 - val_loss: 30.9297 - batch: 22.0000\u001b[0m\n",
      "\u001b[34mEpoch 24/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 22.5518#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 13ms/step - loss: 26.8455 - val_loss: 30.2250 - batch: 23.0000\u001b[0m\n",
      "\u001b[34mEpoch 25/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 30.6607#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 12ms/step - loss: 29.0577 - val_loss: 26.9697 - batch: 24.0000\u001b[0m\n",
      "\u001b[34mEpoch 26/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 26.3002#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 12ms/step - loss: 24.4762 - val_loss: 25.2846 - batch: 25.0000\u001b[0m\n",
      "\u001b[34mEpoch 27/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 19.1893#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 12ms/step - loss: 24.9766 - val_loss: 25.7134 - batch: 26.0000\u001b[0m\n",
      "\u001b[34mEpoch 28/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 27.7781#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 11ms/step - loss: 23.6548 - val_loss: 27.1128 - batch: 27.0000\u001b[0m\n",
      "\u001b[34mEpoch 29/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 14.4045#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 11ms/step - loss: 23.2880 - val_loss: 96.7224 - batch: 28.0000\u001b[0m\n",
      "\u001b[34mEpoch 30/30\u001b[0m\n",
      "\u001b[34m#0151/4 [======>.......................] - ETA: 0s - loss: 123.8904#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/4 [==============================] - 0s 11ms/step - loss: 57.7929 - val_loss: 26.0749 - batch: 29.0000\u001b[0m\n",
      "\u001b[34m1/1 - 0s - loss: 26.0749\n",
      "\u001b[0m\n",
      "\u001b[34mTest MSE : 26.074949264526367\u001b[0m\n",
      "\u001b[34m2021-05-18 23:38:13.344490: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-05-18 23:38:13.344659: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2021-05-18 23:38:13.385073: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\u001b[0m\n",
      "\u001b[34m2021-05-18 23:38:17.613722: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets written to: /opt/ml/model/1/assets\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets written to: /opt/ml/model/1/assets\n",
      "\u001b[0m\n",
      "\u001b[34m2021-05-18 23:38:18,239 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-05-18 23:38:26 Uploading - Uploading generated training model\n",
      "2021-05-18 23:38:26 Completed - Training job completed\n",
      "Training seconds: 49\n",
      "Billable seconds: 49\n"
     ]
    }
   ],
   "source": [
    "hosted_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All SageMaker training jobs produce a model saved in S3 that we can retrieve.  This is an example of the modularity of SageMaker: having trained the model in SageMaker, you can now take the model out of SageMaker and run it anywhere else.  Alternatively, you can deploy the model into a production-ready environment using SageMaker's hosted endpoints functionality, as shown in the **SageMaker hosted endpoint** section below, or in a batch job using SageMaker Batch Transform or SageMaker Processing.\n",
    "\n",
    "Retrieving the model from S3 is very easy:  the hosted training estimator you created above stores a reference to the model's location in S3.  You simply copy the model from S3 using the estimator's `model_data` property and unzip it to inspect the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {hosted_estimator.model_data} ./model/model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unzipped archive should include the assets required by TensorFlow Serving to load the model and serve it, including a .pb file:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf ./model/model.tar.gz -C ./model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Model Tuning (optional) <a class=\"anchor\" id=\"AutomaticModelTuning\">\n",
    "\n",
    "So far we have simply run individual training jobs without any real attempt to tune hyperparameters to produce a better model.  Selecting the right hyperparameter values to train your model can be difficult, and typically is very time consuming if done manually. The right combination of hyperparameters is dependent on your data and algorithm; some algorithms have many different hyperparameters that can be tweaked; some are very sensitive to the hyperparameter values selected; and most have a non-linear relationship between model fit and hyperparameter values.  SageMaker Automatic Model Tuning helps automate the hyperparameter tuning process:  it runs multiple training jobs with different hyperparameter combinations to find the set with the best model performance.\n",
    "\n",
    "We begin by specifying the hyperparameters we wish to tune, and the range of values over which to tune each one.  We also must specify an objective metric to be optimized:  in this use case, we'd like to minimize the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "# hyperparameter_ranges = {\n",
    "#   'learning_rate': ContinuousParameter(0.001, 0.2, scaling_type=\"Logarithmic\"),\n",
    "#   'epochs': IntegerParameter(10, 50),\n",
    "#   'batch_size': IntegerParameter(64, 256),\n",
    "# }\n",
    "\n",
    "# metric_definitions = [{'Name': 'loss',\n",
    "#                        'Regex': ' loss: ([0-9\\\\.]+)'},\n",
    "#                      {'Name': 'val_loss',\n",
    "#                        'Regex': ' val_loss: ([0-9\\\\.]+)'}]\n",
    "\n",
    "# objective_metric_name = 'val_loss'\n",
    "# objective_type = 'Minimize'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we specify a HyperparameterTuner object that takes the above definitions as parameters.  Each tuning job must be given a budget:  a maximum number of training jobs.  A tuning job will complete after that many training jobs have been executed.  \n",
    "\n",
    "We also can specify how much parallelism to employ, in this case five jobs, meaning that the tuning job will complete after three series of five jobs in parallel have completed.  For the default Bayesian Optimization tuning strategy used here, the tuning search is informed by the results of previous groups of training jobs, so we don't run all of the jobs in parallel, but rather divide the jobs into groups of parallel jobs.  There is a trade-off: using more parallel jobs will finish tuning sooner, but likely will sacrifice tuning search accuracy. \n",
    "\n",
    "Now we can launch a hyperparameter tuning job by calling the `fit` method of the HyperparameterTuner object.  The tuning job may take around 10 minutes to finish.  While you're waiting, the status of the tuning job, including metadata and results for invidual training jobs within the tuning job, can be checked in the SageMaker console in the **Hyperparameter tuning jobs** panel.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner = HyperparameterTuner(hosted_estimator,\n",
    "#                             objective_metric_name,\n",
    "#                             hyperparameter_ranges,\n",
    "#                             metric_definitions,\n",
    "#                             max_jobs=15,\n",
    "#                             max_parallel_jobs=5,\n",
    "#                             objective_type=objective_type)\n",
    "\n",
    "# tuning_job_name = \"tf-2-workflow-{}\".format(strftime(\"%d-%H-%M-%S\", gmtime()))\n",
    "# tuner.fit(inputs, job_name=tuning_job_name)\n",
    "# tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the tuning job is finished, we can use the `HyperparameterTuningJobAnalytics` object from the SageMaker Python SDK to list the top 5 tuning jobs with the best performance. Although the results vary from tuning job to tuning job, the best validation loss from the tuning job (under the FinalObjectiveValue column) likely will be substantially lower than the validation loss from the hosted training job above, where we did not perform any tuning other than manually increasing the number of epochs once.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner_metrics = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name)\n",
    "# tuner_metrics.dataframe().sort_values(['FinalObjectiveValue'], ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total training time and training jobs status can be checked with the following lines of code. Because automatic early stopping is by default off, all the training jobs should be completed normally.  For an example of a more in-depth analysis of a tuning job, see the SageMaker official sample [HPO_Analyze_TuningJob_Results.ipynb](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/analyze_results/HPO_Analyze_TuningJob_Results.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_time = tuner_metrics.dataframe()['TrainingElapsedTimeSeconds'].sum() / 3600\n",
    "# print(\"The total training time is {:.2f} hours\".format(total_time))\n",
    "# tuner_metrics.dataframe()['TrainingJobStatus'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SageMaker hosted endpoint (optional) <a class=\"anchor\" id=\"SageMakerHostedEndpoint\">\n",
    "\n",
    "Assuming the best model from the tuning job is better than the model produced by the individual hosted training job above, we could now easily deploy that model to production.  A convenient option is to use a SageMaker hosted endpoint, which serves real time predictions from the trained model (For asynchronous, offline predictions on large datasets, you can use either SageMaker Processing or SageMaker Batch Transform.). The endpoint will retrieve the TensorFlow SavedModel created during training and deploy it within a SageMaker TensorFlow Serving container. This all can be accomplished with one line of code.  \n",
    "\n",
    "More specifically, by calling the `deploy` method of the HyperparameterTuner object we instantiated above, we can directly deploy the best model from the tuning job to a SageMaker hosted endpoint.  It will take a couple of minutes longer to deploy the model to the hosted endpoint compared to a prototyping tool such as a Local Mode endpoint, which is more useful for fast prototyping of inference code.  This is due to the fact that creating a SageMaker hosted endpoint involves spinning up one or more new instances, pulling the relevant inference Docker container and  model, in a production-ready environment prepared to serve traffic, potentially with autoscaling to handle spiky traffic patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tuning_predictor = tuner.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')  # use this if you ran hyperparameter tuning \n",
    "# tuning_predictor = hosted_estimator.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate predictions from this endpoint using the `predict` method of the Predictor object we instantiated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = tuning_predictor.predict(x_test[:10])['predictions'] \n",
    "# flat_list = [float('%.1f'%(item)) for sublist in results for item in sublist]\n",
    "# print('predictions: \\t{}'.format(np.array(flat_list)))\n",
    "# print('target values: \\t{}'.format(y_test[:10].round(decimals=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid billing charges from stray resources, you can delete the prediction endpoint to release its associated instance(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess.delete_endpoint(tuning_predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Automation with SageMaker Pipelines <a class=\"anchor\" id=\"WorkflowAutomation\">\n",
    "\n",
    "In the previous parts of this notebook, we prototyped various steps of a TensorFlow project within the notebook itself, with some steps being run on external SageMaker resources (data preprocessing, hosted training, model tuning, hosted endpoints).  Notebooks are great for prototyping, but generally are  not used in production-ready machine learning pipelines.  \n",
    "\n",
    "A very simple pipeline in SageMaker includes processing the dataset to get it reading for training, performing the actual training, and then using the model to perform some form of inference such as batch prediction (scoring). We'll use SageMaker Pipelines to automate these steps, keeping the pipeline simple for now: it easily can be extended into a far more complex pipeline with conditional steps and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline parameters <a class=\"anchor\" id=\"PipelineParameters\">\n",
    "\n",
    "Before we begin to create the pipeline itself, we should think about how to parameterize it.  For example, we may use different instance types for different purposes, such as CPU-based types for data processing and GPU-based or more powerful types for model training.  These are all \"knobs\" of the pipeline that we can parameterize.  Parameterizing enables custom pipeline executions and schedules without having to modify the pipeline definition.\n",
    "    \n",
    "In the next cell several different pipeline parameters are specified.  These include not only package versions, but also input data location and hardware types and counts.  Many others are possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString\n",
    "\n",
    "# package versions\n",
    "sklearn_version = ParameterString(name=\"SKLearnVersion\", default_value=\"0.23-1\")\n",
    "tensorflow_version = ParameterString(name=\"TensorFlowVersion\", default_value=\"2.3.1\")\n",
    "python_version = ParameterString(name=\"PythonVersion\", default_value=\"py37\")\n",
    "\n",
    "# raw input data\n",
    "input_data = ParameterString(name=\"InputData\", default_value=raw_s3)\n",
    "\n",
    "# processing step parameters\n",
    "processing_instance_type = ParameterString(name=\"ProcessingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=2)\n",
    "\n",
    "# training step parameters\n",
    "training_instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.c5.2xlarge\")\n",
    "training_instance_count = ParameterInteger(name=\"TrainingInstanceCount\", default_value=1)\n",
    "\n",
    "# batch inference step parameters\n",
    "batch_instance_type = ParameterString(name=\"BatchInstanceType\", default_value=\"ml.c5.xlarge\")\n",
    "batch_instance_count = ParameterInteger(name=\"BatchInstanceCount\", default_value=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Step <a class=\"anchor\" id=\"ProcessingStep\">\n",
    "\n",
    "The first step in the pipeline will preprocess the data to prepare it for training. We create a `SKLearnProcessor` object similar to the one above, but now parameterized so we can separately track and change the job configuration as needed, for example to increase the instance type size and count to accommodate a growing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=sklearn_version.default_value,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"tf-2-workflow-process\",\n",
    "    sagemaker_session=sess,\n",
    "    role=role )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"TF2Process\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\", s3_data_distribution_type='ShardedByS3Key'),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    code=\"./preprocessing.py\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Model Creation Steps <a class=\"anchor\" id=\"TrainingModelCreation\">\n",
    "\n",
    "The following code sets up a pipeline step for a training job. As mentioned above, the pipeline is parameterized to specify which SageMaker prebuilt TensorFlow 2 training container to use for the job.  As newer TensorFlow versions are released, the pipeline can be re-executed with the newer versions simply by updating the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "image_uri_train = sagemaker.image_uris.retrieve(\n",
    "                                        framework=\"tensorflow\",\n",
    "                                        region=region,\n",
    "                                        version=tensorflow_version.default_value,\n",
    "                                        py_version=python_version.default_value,\n",
    "                                        instance_type=training_instance_type,\n",
    "                                        image_scope=\"training\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify an `Estimator` object, and define a `TrainingStep` to insert the training job in the pipeline with inputs from the previous SageMaker Processing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model_path = f\"s3://{bucket}/TF2WorkflowTrain\"\n",
    "training_parameters = {'epochs': 44, 'batch_size': 128, 'learning_rate': 0.0125, 'for_pipeline': 'true'}\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    image_uri=image_uri_train,\n",
    "    git_config=git_config,\n",
    "    source_dir='day_2/tf2_smpipelines/train_model',\n",
    "    entry_point='train.py',\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=training_instance_count,\n",
    "    role=role,\n",
    "    base_job_name=\"tf-2-workflow-train\",\n",
    "    output_path=model_path,\n",
    "    hyperparameters=training_parameters )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_train = TrainingStep(\n",
    "    name=\"TF2WorkflowTrain\",\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri\n",
    "        ),\n",
    "        \"test\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another step, we create a SageMaker `Model` object to wrap the model artifact, and associate it with a separate SageMaker prebuilt TensorFlow Serving inference container to potentially use later, for example in a SageMaker hosted endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "image_uri_inference = sagemaker.image_uris.retrieve(\n",
    "                                        framework=\"tensorflow\",\n",
    "                                        region=region,\n",
    "                                        version=tensorflow_version.default_value,\n",
    "                                        py_version=python_version.default_value,\n",
    "                                        instance_type=batch_instance_type,\n",
    "                                        image_scope=\"inference\" )\n",
    "model = Model(\n",
    "    image_uri=image_uri_inference,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sess,\n",
    "    role=role )\n",
    "\n",
    "inputs_model = CreateModelInput(\n",
    "    instance_type=batch_instance_type )\n",
    "\n",
    "step_create_model = CreateModelStep(\n",
    "    name=\"TF2WorkflowCreateModel\",\n",
    "    model=model,\n",
    "    inputs=inputs_model )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Scoring Step <a class=\"anchor\" id=\"BatchScoringStep\">\n",
    "    \n",
    "The final step in this pipeline is offline, batch scoring (inference/prediction).  The inputs to this step will be the model we trained earlier, and the test data.  A simple, ordinary Python script is all we need to do the actual batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting batch-score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile batch-score.py\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import tarfile\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    install('tensorflow==2.3.1')\n",
    "    model_path = f\"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path, 'r:gz') as tar:\n",
    "        tar.extractall('./model')\n",
    "    import tensorflow as tf\n",
    "    model = tf.keras.models.load_model('./model/1')\n",
    "    test_path = \"/opt/ml/processing/test/\"\n",
    "    x_test = np.load(os.path.join(test_path, 'x_test.npy'))\n",
    "    y_test = np.load(os.path.join(test_path, 'y_test.npy'))\n",
    "    scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "    print(\"\\nTest MSE :\", scores)\n",
    "    \n",
    "    output_dir = \"/opt/ml/processing/batch\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    evaluation_path = f\"{output_dir}/score-report.txt\"\n",
    "    with open(evaluation_path, 'w') as writer:\n",
    "         writer.write(f\"Test MSE : {scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regard to the SageMaker features we could use to perform batch scoring, we have several choices, including SageMaker Processing and SageMaker Batch Transform.  We'll use SageMaker Processing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_scorer = SKLearnProcessor(\n",
    "                    framework_version=sklearn_version.default_value,\n",
    "                    instance_type=batch_instance_type,\n",
    "                    instance_count=batch_instance_count,\n",
    "                    base_job_name=\"tf-2-workflow-batch\",\n",
    "                    sagemaker_session=sess,\n",
    "                    role=role )\n",
    "\n",
    "step_batch = ProcessingStep(\n",
    "                    name=\"TF2WorkflowBatchScoring\",\n",
    "                    processor=batch_scorer,\n",
    "                    inputs=[\n",
    "                        ProcessingInput(\n",
    "                            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "                            destination=\"/opt/ml/processing/model\"\n",
    "                        ),\n",
    "                        ProcessingInput(\n",
    "                            source=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                                \"test\"\n",
    "                            ].S3Output.S3Uri,\n",
    "                            destination=\"/opt/ml/processing/test\"\n",
    "                        )\n",
    "                    ],\n",
    "                    outputs=[\n",
    "                        ProcessingOutput(output_name=\"batch\", source=\"/opt/ml/processing/batch\"),\n",
    "                    ],\n",
    "                    code=\"./batch-score.py\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and executing the pipeline <a class=\"anchor\" id=\"CreatingExecutingPipeline\">\n",
    "\n",
    "With all of the pipeline steps now defined, we can define the pipeline itself as a `Pipeline` object comprising a series of those steps.  Parallel and conditional steps also are possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=f\"TF2Workflow\",\n",
    "    parameters=[sklearn_version,\n",
    "                tensorflow_version,\n",
    "                python_version,\n",
    "                input_data,\n",
    "                processing_instance_type, \n",
    "                processing_instance_count, \n",
    "                training_instance_type, \n",
    "                training_instance_count,\n",
    "                batch_instance_type,\n",
    "                batch_instance_count],\n",
    "    steps=[step_process, \n",
    "           step_train, \n",
    "           step_create_model,\n",
    "           step_batch\n",
    "          ],\n",
    "    sagemaker_session=sess )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the pipeline definition in JSON format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [{'Name': 'SKLearnVersion',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': '0.23-1'},\n",
       "  {'Name': 'TensorFlowVersion', 'Type': 'String', 'DefaultValue': '2.3.1'},\n",
       "  {'Name': 'PythonVersion', 'Type': 'String', 'DefaultValue': 'py37'},\n",
       "  {'Name': 'InputData',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://sagemaker-us-east-1-154082585954/tf-2-workflow/data/raw'},\n",
       "  {'Name': 'ProcessingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.xlarge'},\n",
       "  {'Name': 'ProcessingInstanceCount', 'Type': 'Integer', 'DefaultValue': 2},\n",
       "  {'Name': 'TrainingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.c5.2xlarge'},\n",
       "  {'Name': 'TrainingInstanceCount', 'Type': 'Integer', 'DefaultValue': 1},\n",
       "  {'Name': 'BatchInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.c5.xlarge'},\n",
       "  {'Name': 'BatchInstanceCount', 'Type': 'Integer', 'DefaultValue': 1}],\n",
       " 'Steps': [{'Name': 'TF2Process',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': {'Get': 'Parameters.ProcessingInstanceType'},\n",
       "      'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/preprocessing.py']},\n",
       "    'RoleArn': 'arn:aws:iam::154082585954:role/service-role/AmazonSageMaker-ExecutionRole-12345',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': 'Parameters.InputData'},\n",
       "       'LocalPath': '/opt/ml/processing/input',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'ShardedByS3Key',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-154082585954/tf-2-workflow-process-2021-05-18-23-41-28-871/input/code/preprocessing.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-154082585954/tf-2-workflow-process-2021-05-18-23-41-28-871/output/train',\n",
       "        'LocalPath': '/opt/ml/processing/train',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'test',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-154082585954/tf-2-workflow-process-2021-05-18-23-41-28-871/output/test',\n",
       "        'LocalPath': '/opt/ml/processing/test',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}}},\n",
       "  {'Name': 'TF2WorkflowTrain',\n",
       "   'Type': 'Training',\n",
       "   'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "     'TrainingImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.3.1-cpu-py37'},\n",
       "    'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-154082585954/TF2WorkflowTrain'},\n",
       "    'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "    'ResourceConfig': {'InstanceCount': {'Get': 'Parameters.TrainingInstanceCount'},\n",
       "     'InstanceType': {'Get': 'Parameters.TrainingInstanceType'},\n",
       "     'VolumeSizeInGB': 30},\n",
       "    'RoleArn': 'arn:aws:iam::154082585954:role/service-role/AmazonSageMaker-ExecutionRole-12345',\n",
       "    'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.TF2Process.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ChannelName': 'train'},\n",
       "     {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.TF2Process.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ChannelName': 'test'}],\n",
       "    'HyperParameters': {'epochs': '44',\n",
       "     'batch_size': '128',\n",
       "     'learning_rate': '0.0125',\n",
       "     'for_pipeline': '\"true\"',\n",
       "     'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-154082585954/tf-2-workflow-train-2021-05-18-23-41-28-976/source/sourcedir.tar.gz\"',\n",
       "     'sagemaker_program': '\"train.py\"',\n",
       "     'sagemaker_container_log_level': '20',\n",
       "     'sagemaker_job_name': '\"tf-2-workflow-train-2021-05-18-23-41-28-976\"',\n",
       "     'sagemaker_region': '\"us-east-1\"',\n",
       "     'model_dir': '\"s3://sagemaker-us-east-1-154082585954/TF2WorkflowTrain/tf-2-workflow-train-2021-05-18-23-41-28-976/model\"'},\n",
       "    'DebugHookConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-154082585954/TF2WorkflowTrain',\n",
       "     'CollectionConfigurations': []},\n",
       "    'ProfilerRuleConfigurations': [{'RuleConfigurationName': 'ProfilerReport-1621381288',\n",
       "      'RuleEvaluatorImage': '503895931360.dkr.ecr.us-east-1.amazonaws.com/sagemaker-debugger-rules:latest',\n",
       "      'RuleParameters': {'rule_to_invoke': 'ProfilerReport'}}],\n",
       "    'ProfilerConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-154082585954/TF2WorkflowTrain'}}},\n",
       "  {'Name': 'TF2WorkflowCreateModel',\n",
       "   'Type': 'Model',\n",
       "   'Arguments': {'ExecutionRoleArn': 'arn:aws:iam::154082585954:role/service-role/AmazonSageMaker-ExecutionRole-12345',\n",
       "    'PrimaryContainer': {'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.3.1-cpu',\n",
       "     'Environment': {},\n",
       "     'ModelDataUrl': {'Get': 'Steps.TF2WorkflowTrain.ModelArtifacts.S3ModelArtifacts'}}}},\n",
       "  {'Name': 'TF2WorkflowBatchScoring',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': {'Get': 'Parameters.BatchInstanceType'},\n",
       "      'InstanceCount': {'Get': 'Parameters.BatchInstanceCount'},\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/batch-score.py']},\n",
       "    'RoleArn': 'arn:aws:iam::154082585954:role/service-role/AmazonSageMaker-ExecutionRole-12345',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': 'Steps.TF2WorkflowTrain.ModelArtifacts.S3ModelArtifacts'},\n",
       "       'LocalPath': '/opt/ml/processing/model',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'input-2',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': \"Steps.TF2Process.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri\"},\n",
       "       'LocalPath': '/opt/ml/processing/test',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-154082585954/tf-2-workflow-batch-2021-05-18-23-41-29-660/input/code/batch-score.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'batch',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-154082585954/tf-2-workflow-batch-2021-05-18-23-41-29-660/output/batch',\n",
       "        'LocalPath': '/opt/ml/processing/batch',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}}}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After upserting its definition, we can start the pipeline with the `Pipeline` object's `start` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now confirm that the pipeline is executing.  In the log output below, confirm that `PipelineExecutionStatus` is `Executing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:154082585954:pipeline/tf2workflow',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:154082585954:pipeline/tf2workflow/execution/x05hbvbtugh9',\n",
       " 'PipelineExecutionDisplayName': 'execution-1621381292567',\n",
       " 'PipelineExecutionStatus': 'Executing',\n",
       " 'CreationTime': datetime.datetime(2021, 5, 18, 23, 41, 32, 389000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2021, 5, 18, 23, 41, 32, 389000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:154082585954:user-profile/d-tvjg2ugulkio/user-154082585954-1',\n",
       "  'UserProfileName': 'user-154082585954-1',\n",
       "  'DomainId': 'd-tvjg2ugulkio'},\n",
       " 'LastModifiedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:154082585954:user-profile/d-tvjg2ugulkio/user-154082585954-1',\n",
       "  'UserProfileName': 'user-154082585954-1',\n",
       "  'DomainId': 'd-tvjg2ugulkio'},\n",
       " 'ResponseMetadata': {'RequestId': '739c920e-cf80-460a-a972-b3029bf28fce',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '739c920e-cf80-460a-a972-b3029bf28fce',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '737',\n",
       "   'date': 'Tue, 18 May 2021 23:41:32 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically this pipeline should take about 10 minutes to complete.  We can wait for completion by invoking `wait()`. After execution is complete, we can list the status of the pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait()\n",
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the score report\n",
    "\n",
    "After the batch scoring job in the pipeline is complete, the batch scoring report is uploaded to S3.  For simplicity, this report simply states the test mean squared error (MSE), but in general reports can include as much detail as desired.  Reports such as these also can be formatted for use in conditional approval steps in SageMaker Pipelines.  For example, the pipeline could have a condition step that only allows further steps to proceed only if the MSE is lower than some threshold.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_path = f\"{step_batch.outputs[0].destination}/score-report.txt\"\n",
    "!aws s3 cp {report_path} ./score-report.txt && cat score-report.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Lineage Tracking <a class=\"anchor\" id=\"LineageOfPipelineArtifacts\">\n",
    "\n",
    "SageMaker ML Lineage Tracking creates and stores information about the steps of a ML workflow from data preparation to model deployment. With the tracking information you can reproduce the workflow steps, track model and dataset lineage, and establish model governance and audit standards.\n",
    "\n",
    "Let's now check out the lineage of the model generated by the pipeline above.  The lineage table identifies the resources used in training, including the timestamped train and test data sources, and the specific version of the TensorFlow 2 container in use during the training job.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker.session.Session())\n",
    "\n",
    "for execution_step in reversed(execution.list_steps()):\n",
    "    if execution_step['StepName'] == 'TF2WorkflowTrain':\n",
    "        display(viz.show(pipeline_execution_step=execution_step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions <a class=\"anchor\" id=\"Extensions\">\n",
    "\n",
    "We've covered a lot of content in this notebook:  SageMaker Processing for data transformation, prototyping training and inference code, Automatic Model Tuning, and SageMaker hosted training and inference.  These are central elements for most machine learning workflows in SageMaker.  Additionally, we examined how SageMaker Pipelines helps automate deep learning workflows after completion of the prototyping phase of a project.\n",
    "\n",
    "Besides all of the SageMaker features explored above, there are many other features that may be applicable to your project.  For example, to handle common problems during model training such as vanishing or exploding gradients, **SageMaker Debugger** is useful.  To manage common problems such as data drift after a model is in production, **SageMaker Model Monitor** can be applied."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.3-cpu-py37-ubuntu18.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
